---
title: "MovieLens Project Report"
author: "Raj Krishna, PMP, PMI-ACP, CSM"
date: "`r Sys.Date()`"
output:
  pdf_document: 
  word_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_knit$set(progress = TRUE, aliases = c(h = "fig.height", w = "fig.width"))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, cache.lazy = FALSE)
if(!require(formatR)) 
  install.packages("formatR", repos = "http://cran.us.r-project.org")

```

# Introduction  
The objective of the MovieLens project is to build a recommendation system using the MovieLens dataset. The MovieLens dataset was created by the [GroupLens](https://grouplens.org/), a research lab in the Department of Computer Science and Engineering at the University of Minnesota.  

The full dataset contains 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users[^1]. For the purpose of this project we would be using a smaller stable benchmark [10M movie ratings](https://grouplens.org/datasets/movielens/10m/) dataset that contains 10,000,054 ratings and 95,580 tags applied to 10,681 movies by 71,567 users of the online movie recommender service [MovieLens](https://movielens.org/).  

The MovieLens dataset is split into two datasets; the edx dataset, used to train the algorithms and predict the movie ratings in, the second, the validation dataset. Root Mean Squared Error (RMSE), also known as Residual Mean Squared Error, is used to gauge the accuracy of the said predictions. To avoid overtraining, the validation dataset is used only to report the **final** RMSE.

During the course of this project, various Machine Learning models would be assessed and their respective RMSEs would be compared to arrive at the final recommended model which would then be used to report the final RMSE using the validation dataset. The goal is to achieve **RMSE $\le$ 0.8649**

# Datasets  
The MovieLens dataset is downloaded and split into **edx** and **validation** datasets. edx dataset contains 90% of the MovieLens dataset while the validation dataset contains 10%. Each of the datasets has 6 components namely *userId*, *movieId*, *rating*, *timestamp*, *title*, and *genres*. During this process we'll also ensure that the *userID* and *movieId* in the validation dataset are also present in the edx dataset.

## Overview  
Following code is used to download the MovieLens dataset and split it into two:

### Load necessary libraries

```{r library-load, message=FALSE, include=TRUE}
##################################
# Create edx set, validation set #
##################################

# Note: this process could take a couple of minutes
# Installing/loading libraries necessary for the project and the report
if(!require(tidyverse)) 
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) 
  install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) 
  install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) 
  install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) 
  install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(knitr)) 
  install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(gghighlight)) 
  install.packages("gghighlight", repos = "http://cran.us.r-project.org")

```

### Download 10M MovieLens dataset and split to create edx and validation sets

```{r dataset-creation, message=FALSE, cache=TRUE, include=TRUE}
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip


dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", 
                             readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% 
  mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
# if using R 3.5 or earlier, use `set.seed(1)` instead
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
```

### Remove temporary objects
  
```{r remove-objects, include=TRUE}
# Remove the objects that are not needed for further processing
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

Both edx and validation datasets contain the following 6 variables:

1. **userId**: Anonymizes MovieLens user ids
2. **movieId**: MovieLens movie id
3. **rating**: Rating given by individual user and is made on a 5-star scale, with half-star increments 
4. **timestamp**: Represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970
5. **title**: Title of the movie
6. **genre**: Pipe-separated list of genre(s) the movie belongs to

Each row represents the rating given to a single movie by a single user

### edx
```{r edx, include=TRUE}
glimpse(edx, 85)
```

As depicted by the snippet above, edx dataset contains ***9,000,055*** rows and 6 variables. The edx dataset would be further split into various train and test sets in order to train the various models and predict the ratings($Y$).

### validation
```{r validation, include=TRUE}
glimpse(validation, 85)
```
The validation dataset contains ***999,999*** rows and 6 variables. The validation dataset would only be used to obtain the final prediction and the RMSE therein. 

## Additional Datasets
As part of the assessment process, various additional datasets were necessitated. For ease of reference and in order to speed up code and reduce the processing time, they are being created and listed below. Where data partition is created, care has been taken to ensure that the *userID* and *movieId* in one partition is also present in the other.  

```{r create-train-test-set, include=TRUE, cache=TRUE, message=FALSE}
set.seed(1, sample.kind="Rounding")
# creating additional train and test set from edx data
# obtaining test index with 20% of the edx data
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
temp <- edx[test_index, ]
train_set <- edx[-test_index, ]

# Ensuring movieId and userId in train set are also in test set
test_set <- temp %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Add rows removed from test set back into train set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)
```
### 1. train_set
```{r dim-train-set, include=TRUE}
dim(train_set)
```
The **train_set** Contains 80% of the edx dataset. The train_set is used to train various models and predict ratings for the test set.  

### 2. test_set
```{r dim-test-set, include=TRUE}
dim(test_set)
```
The **test_set** contains 20% of the edx dataset. The test_set is used to validate ratings predicted using train_set and to calculate the RMSE.  

### 3. genre_train_set
```{r create-genre-train-set, cache=TRUE, include=TRUE}
genre_train_set <- train_set %>%
  separate_rows(genres, sep = "\\|")

dim(genre_train_set)
```
The **genre_train_set** is derived from the train_set where the pipe-separated genre variable has been split into separate rows.  

### 4. genre_test_set
```{r create-genre-test-set, cache=TRUE, include=TRUE}
genre_test_set <- test_set %>%
  separate_rows(genres, sep = "\\|")

dim(genre_test_set)
```
The **genre_test_set** is derived from the test_set where the pipe-separated genre variable has been split into separate rows.  

### 5. genre_edx
```{r create-genre-edx, cache=TRUE, include=TRUE}
genre_edx <- edx %>%
  separate_rows(genres, sep = "\\|")

dim(genre_edx)
```
The **genre_edx_set** is derived from edx where the pipe-separated genre variable has been split into separate rows.

### 6. genre_validation
```{r create-genre-validation, cache=TRUE, include=TRUE}
genre_validation <- validation %>%
  separate_rows(genres, sep = "\\|")

dim(genre_validation)
```
The **genre_validation_set** is derived from edx where the pipe-separated genre variable has been split into separate rows.

## Data Exploration  
Before we dive deep into the project, let's explore the data that we have a bit. We'll be using the edx data set for this purpose.  

### Number of distinct users and movies
Let's first see how many distinct users and movies are there in the dataset. 
```{r number-of-users-movies, cache = TRUE, include = TRUE}
edx %>%
  group_by(userId) %>%
  summarise(n_distinct(userId)) %>% count() %>% kable()
  
edx %>% 
  group_by(movieId) %>%
  summarise(n_distinct(movieId)) %>% count() %>% kable()

```

### Average ratings by User  

```{r average-user-rating, cache = TRUE, include = TRUE}

# Plot to show the average rating by user 
edx %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(aes(b_u),bins = 20, fill = "#333333", color = "white") +
  scale_x_continuous(limits = c(2, 5)) +
  labs(x = "Average Rating", y = "Number of ratings",
       title = "Average ratings by User",
       caption = "source data: edx dataset") +
  theme(plot.title = element_text(color = "red", face = "bold", hjust = 0.5))
```

We can see variablity in the rating based on the users. Some users give high ratings to movies while others seem more cautious with their ratings. This would be another effect that we should be analyzing.  

### Average rating by Movie  

```{r average-movie-rating, cache = TRUE, include = TRUE}

# Plot to show the average rating by movie 
edx %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating)) %>%
  ggplot(aes(b_i)) + 
  geom_histogram(aes(b_i),bins = 20, fill = "#333333", color = "white") +
  labs(x = "Average Rating", y = "Number of ratings",
       title = "Average ratings by Movie",
       caption = "source data: edx dataset") +
  theme(plot.title = element_text(color = "red", face = "bold", hjust = 0.5))
```

As we can see there is variability in the average rating based on the movie. Some movies are rated higher than others. We would explore this effect later in the project.

### Day of the week  
The ratings for a movie could be dependent on when it was rated, let's explore this possibility.  
```{r day-of-the-week, cache = TRUE, include = TRUE}

# Plot to show the number of ratings by Weekday
edx %>%
  mutate(Weekday = lubridate::wday(lubridate::as_datetime(timestamp), 
                                   label = TRUE, abbr = FALSE)) %>%
  group_by(Weekday) %>%
  summarise(n = n()) %>%
  ggplot(aes(reorder(Weekday, n), n)) + 
  coord_flip(y = c(400000, 1500000)) +
  geom_bar(stat = "identity", fill ="#333333", color = "white") + 
  labs(x = "Day of the week", y = "Number of ratings",
       title = "Number of ratings by day of the week",
       caption = "source data: edx dataset") + 
  geom_label(aes(label = n),fill = "red", color = "white") +
  theme(plot.title = element_text(color = "red", face = "bold", hjust = 0.5))

```

Interestingly, most movie ratings seem to have been given on a Tuesday. Now, let's see if there is any impact of the day of the week when the movie was rated on the rating itself.  

```{r average-rating-by-weekday, cache = TRUE, include = TRUE}

# Plot to show the average ratings by weekday
edx %>%
  mutate(Weekday = lubridate::wday(lubridate::as_datetime(timestamp), 
                                 label = TRUE, abbr = FALSE)) %>%
  group_by(Weekday) %>%
  summarise(avg_rating = round(mean(rating),2)) %>%
  ggplot(aes(reorder(Weekday, avg_rating), avg_rating)) + 
  coord_flip(y = c(3.45, 3.55)) +
  geom_bar(stat = "identity", fill ="#333333", color = "white") + 
  labs(x = "Day of the week", y = "Number of ratings",
       title = "Average rating by day of the week",
       caption = "source data: edx dataset") + 
  geom_label(aes(label = avg_rating),fill = "red", color = "white") +
  theme(plot.title = element_text(color = "red", face = "bold", hjust = 0.5))


```

As we can see there does seem to be some impact due to the day of the week on the ratings, but it may not be significant. We'll explore related impact further in the course of our project.  

### Genre of the movie  
Let's now explore the data based on the genre and see if we can identify some variability which we can use in our model. We'd be using the *genre_edx*, dataset that we created by splitting pipe-separated genre data, for this purpose.

First let's see how many movies do we have by each genre.

```{r genre-count, cache = TRUE, include = TRUE}
# Code to list out the number of movies by Genre
genre_edx %>%
  group_by(genres) %>%
  summarise(Movies = n()) %>%
  rename(Genre = genres) %>%
  arrange(desc(Movies)) %>%
  knitr::kable("latex", caption = "Number of Movies by Genre", 
               escape = FALSE, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = F , position = "center") %>%
  row_spec(0, bold = TRUE, color = "white" , background ="red") %>%
  footnote(general = "genre edx dataset", 
           general_title = "Source Data:", footnote_as_chunk = TRUE)
  
```

Let's now identify the average rating by genre

```{r genre-average, cache = TRUE, include = TRUE}
# Plot to show the average rating by Genre
genre_edx %>%
  group_by(genres) %>%
  summarise(avg_rating = round(mean(rating), 2)) %>%
  ggplot(aes(reorder(genres, avg_rating), avg_rating)) + 
  coord_flip(y = c(3.0, 4.10)) +
  geom_bar(stat = "identity", fill ="#333333", color = "white") + 
  labs(x = "Genre", y = " Average ratings",
       title = "Average rating by Genre",
       caption = "source data: genre edx dataset") + 
  geom_label(aes(label = avg_rating), size = 2.5, 
             fontface = "bold", fill = "red", color = "white") +
  theme(plot.title = element_text(color = "red", face = "bold", hjust = 0.5))

```
As we can see there seems to be a relationship between the rating and the genre of the movie. We would use this insight to further improve our model.  

We now have a good insight into the data that we are dealing with, let's put what we have learned into developing models for our project.  

# Methods and Analysis
Now that the necessary datasets have been setup, we can start exploring the various models that would be built. For the MovieLens projects various ***Linear Regression Models*** would be used. 

## Linear Regression Models
Linear regression is the simplest of the statistical models. In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables)[^2]. 

In Machine Learning Linear Regression can be used to predict a response($Y$) based on one or more predictors($X$). In the simplest of forms it can be mathematically respresented as

$$Y = \beta_0 + \beta_1X$$
where:

* $\beta_0$ The slope of the regression line
* $\beta_1$ The intercept of the regression line


### Naive model  
The first model to be explored assumes the same rating for all movies by all users with any deviation explained by random variation. The model can be represented as

$$Y_{u,i} = \mu + \epsilon_{u,i}$$
where: 

* $\epsilon_{u,i}$ independent errors sampled from the same distribution centred at 0
* $\mu$  the actual rating for all the movies


### Modeling movie effect  
Modeling the movie effect takes into consideration that some movies are rated higher than others, we can thus supplement our model with $b_i$ to represent this bias or effect.

$$Y_{u,i} = \mu + b_{i} + \epsilon_{u,i}$$
where: 

* $\epsilon_{u,i}$ independent errors sampled from the same distribution centred at 0
* $\mu$  the actual rating for all the movies
* $b_{i}$ movie effects or movie bias

For this model we know that the least square estimate $b_i$ is the average of $Y_{u,i} - \mu$ for each movie i as using lm() function would be very slow due to the number $b_i$ that needs to be accounted for. Thus:

$$b_i = Y_{u,i} - \mu$$  

### Modeling movie + user effect  
The above model doesn't account for the bias of a user or the user effect. Thus we can further improve on our model by accounting for $b_u$. So our enhanced model can be represented as

$$Y_{u,i} = \mu + b_{i} + b_{u} + \epsilon_{u,i}$$

where: 

* $\epsilon_{u,i}$ independent errors sampled from the same distribution centred at 0
* $\mu$  the actual rating for all the movies
* $b_{i}$ movie effects or movie bias
* $b_{u}$ user effects or user bias

Follwoing the reasoning in the earlier model we would derive the user effect $b_u$ as the average of $Y_{u,i} - \mu - b_i$. Thus:

$$b_u = Y_{u,i} - \mu - b_i$$  

### Modeling movie + user + time effect  
An additional improvement to the model could be the effect of time on the rating. So if $d_{u,i}$ is defined as the day that user $u$ rates the movie $i$ then the time effect can be defined as per the model below

$$Y_{u,i} = \mu + b_{i} + b_{u} + f(d_{u,i}) + \epsilon_{u,i}$$ with $f$ a smooth function of $d_{u,i}$

where:

* $\epsilon_{u,i}$ independent errors sampled from the same distribution centred at 0
* $\mu$  the actual rating for all the movies
* $b_{i}$ movie effects or movie bias
* $b_{u}$ user effects or user bias
* $d_{u,i}$ the day for user $u$'s rating of movie $i$

And the time effect represented by $b_t$ can be arrived at by obtaining $\mu$, $b_i$, and $b_u$ and calculating the average of $Y_{u,i} - \mu - b_i - b_u$. Thus:

$$b_u = Y_{u,i} - \mu - b_i - b_u$$  

### Modeling movie + user + genre effect   
A similar effect on the rating could be that of genre, which could be derived as per the model below

$$Y_{u,i} = \mu + b_{i} + b_{u} + \sum_{k=1}^Kx_{u,i\beta_{k}} + \epsilon_{u,i}$$ with $x_{u,i}^k$ = 1 if $g_{u,i}$ is genre $k$

where: 

* $\epsilon_{u,i}$ independent errors sampled from the same distribution centred at 0
* $\mu$  the actual rating for all the movies
* $b_{i}$ movie effects or movie bias
* $b_{u}$ user effects or user bias
* $g_{u,i}$ the genre for user $u$'s rating of movie $i$

And the genre effect represented by $b_g$ can be arrived at by obtaining $\mu$, $b_i$, and $b_u$ and calculating the average of $Y_{u,i} - \mu - b_i - b_u$. Thus:

$$b_g = Y_{u,i} - \mu - b_i - b_u$$ 

## Regularization[^3]
Regularization is a regression technique that helps us correct any overfitting in our model. It permits us to penalize large estimates that are induced by small sample sizes. The general idea of the technique is to minimize the variance of effect estimates.

### Regularized movie + user + genre effect  
As we would see in subsequent steps below, the movie + user + genre effect model would provide the least RMSE. We would thus regularize this model in order to improve it.

This regularized model can be defined as below

$$\frac{1}{N}\sum_{u,i}(y_{u,i} - \mu - b_i - b_u - b_g)^2 + \lambda(\sum_ib_i^2 + \sum_ub_u^2 + \sum_gb_g^2)$$  

where $\frac{1}{N} \sum_{u,i}(y_{u,i} - \mu - b_i - b_u - b_g) ^ 2$ is the least squares while $\lambda(\sum_i b_i ^ 2 + \sum_u b_u ^ 2 + \sum_g b_g ^ 2)$ is the penalty. Larger the effect estimates larger the penalty.

We would use cross validation to arrive at the best $\lambda$ that corresponds to the least RMSE. Using calculus it can be derived that the values of $b_i$, $b_u$, and $b_g$ that minimizes the above equation will be

$$b_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u = 1} ^ {n_i}(Y_{u,i} - \mu)$$

$$b_u(\lambda) = \frac{1}{\lambda + n_i} \sum_{u = 1} ^ {n_i} (Y_{u,i} - \mu - b_i)$$

$$b_g(\lambda) = \frac{1}{\lambda + n_i} \sum_{u = 1} ^ {n_i}(Y_{u,i} - \mu - b_i - b_u)$$
where $n_i$ is the number of ratings made for movie $i$.


## RMSE Function

We will evaluate the accuracy of our model by determining by how much our predicted ratings from the respective models, deviate from the actual ratings. 

If $y_{u,i}$ is the rating for movie i by user u, and $\hat{y}_{u,i}$ denotes the predicted rating then the RMSE would be defined as:

$$RMSE = \sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i} - y_{u,i}) ^ 2}$$


with N being the number of user and movie combinations

```{r rmse-definition, include=TRUE}

RMSE <- function(actual_ratings, predicted_ratings){
    sqrt(mean((actual_ratings - predicted_ratings) ^ 2))
  }

```

# Results
We would now assess our models and their performance to arrive at the best model to achieve our objective of an **RMSE $\le$ 0.8649**

## Assessment of the models

### Naive model 

```{r naive-model, include = TRUE}

# Naive Model: RMSE using average movie rating

# Calculate the average
mu <- mean(train_set$rating)

# Evaluate the performance
naive_rmse <- RMSE(test_set$rating, mu)

# Store the result
rmse_results <- data_frame(Method = "Naive model", 
                           RMSE = round(naive_rmse, 7), Improvement = "NA")

# Display the results in a tabular format
rmse_results %>%
   mutate(
    RMSE = cell_spec(RMSE, "latex", color = ifelse(RMSE <=0.8649, "#006600","red"))
  ) %>%
  knitr::kable("latex", caption = "Naive Model", escape = FALSE, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = F , position = "center") %>%
  row_spec(0, bold = TRUE, color = "white" , background ="red")

```

So, we now have our very first model, however the RMSE of **$`r naive_rmse`$** can definitely be improved upon. Let's continue with the assessment of our next model.

### Modeling movie effect

```{r movie-effect-model, include = TRUE}

# Movie Effect Model: Modeling movie effect (b_i)

# Calculate the movie effect (b_i)
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - mu))

# Obtain the prediction of movie ratings
me_rating <- mu + test_set %>%
  left_join(movie_avgs, by = "movieId") %>% # Combining movie averages with the test set
  pull(b_i)

# Evaluate the performance
model_1_rmse <- RMSE(test_set$rating, me_rating)

# Calculate the improvement in percentage
improvement <- round((naive_rmse - model_1_rmse) * 100 / naive_rmse, 4)

# Append the result for visual comparison
rmse_results <- bind_rows(rmse_results, 
                          data_frame(Method = "Movie effect", 
                                     RMSE = round(model_1_rmse, 7),
                                     Improvement = as.character(improvement)))

# Display the results in a tabular format
rmse_results %>%
   mutate(
    RMSE = cell_spec(RMSE, "latex", color = ifelse(RMSE <=0.8649, "#006600","red"))
  ) %>%
  knitr::kable("latex", caption = "Comparison of 2 models", 
               escape = FALSE, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = F , position = "center") %>%
  row_spec(0, bold = TRUE, color = "white" , background ="red") %>%
  footnote(general = "Improvement in percentage", 
           general_title = "Note:", footnote_as_chunk = TRUE)

```

When we account for the movie effect we immediately see an improvement of $`r improvement`$% which is significant, though there is still scope for improvement. Let's move on.

### Modeling movie + user effect

```{r movie-user-effect-model, cache = TRUE, include = TRUE}

# Movie and user effect model: Modeling movie effect and user effect (b_i + b_u)

# Calculate the user effect (b_u)
user_avgs <- train_set %>%
  left_join(movie_avgs, by = "movieId") %>%  # Combining movie averages with the train set
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu - b_i))

# Obtain the prediction of movie ratings
ue_rating <- test_set %>%
  left_join(movie_avgs, by = "movieId") %>% # Combining movie averages with the test set
  left_join(user_avgs, by = "userId") %>% # Combining user averages with the test set
  mutate(uer = mu + b_i + b_u)%>%
  pull(uer)

# Evaluate the performance
model_2_rmse <- RMSE(test_set$rating, ue_rating)

# Calculate the improvement in percentage
improvement <- round((model_1_rmse - model_2_rmse) * 100 / model_1_rmse, 4)

# Append the result for visual comparison
rmse_results <- bind_rows(rmse_results, 
                          data_frame(Method = "Movie + User effect",              
                                     RMSE = round(model_2_rmse, 7),
                                     Improvement = as.character(improvement)))

#Display the results in a tabular format
rmse_results %>%
   mutate(
    RMSE = cell_spec(RMSE, "latex", color = ifelse(RMSE <=0.8649, "#006600","red"))
  ) %>%
  knitr::kable("latex", caption = "Comparison of 3 models", 
               escape = FALSE, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = F , position = "center") %>%
  row_spec(0, bold = TRUE, color = "white" , background ="red") %>%
  footnote(general = "Improvement in percentage", 
           general_title = "Note:", footnote_as_chunk = TRUE)

```

As we can see, including the user effect has further improved our model. There is an improvement of $`r improvement`$% from our previous model. Now let's see how much does time bias impact our model.

### Modeling movie + user + time effect 

```{r movie-user-time-effect-model, cache = TRUE, include = TRUE}

# Movie, user and time effect model: Modeling movie effect, user effect, 
# and time effect (b_i + b_u + b_t)

# Mutate test set to create a date column which would be used for the join 
test_set <- test_set %>%
  mutate(date = lubridate::round_date(lubridate::as_datetime(timestamp), unit = "week"))

# Calculate the time effect (b_t)
time_avgs <- train_set %>%
  left_join(movie_avgs, by = "movieId") %>%  # Combining movie averages with the train set
  left_join(user_avgs, by = "userId") %>% # Combining user averages with the train set
  mutate(date = round_date(as_datetime(timestamp), unit = "week")) %>%
  group_by(date) %>%
  summarise(b_t = mean(rating - mu - b_i - b_u))

# Obtain the prediction of movie ratings
te_rating <- test_set %>%
  left_join(movie_avgs, by = "movieId") %>%  # Combining movie averages with the test set
  left_join(user_avgs, by = "userId") %>% # Combining user averages with the test set
  left_join(time_avgs, by = "date") %>%
  mutate(ter = mu + b_i + b_u + b_t)%>%
  pull(ter)

# Evaluate the performance
model_3_rmse <- RMSE(test_set$rating, te_rating)

# Calculate the improvement in percentage
improvement = round((model_2_rmse - model_3_rmse) * 100 / model_2_rmse, 4)

# Append the result for visual comparison
rmse_results <- bind_rows(rmse_results, 
                          data_frame(Method = "Movie + User + Time effect",
                                     RMSE = round(model_3_rmse, 7),
                                     Improvement = as.character(improvement)))

#Display the results in a tabular format
rmse_results %>%
   mutate(
    RMSE = cell_spec(RMSE, "latex", color = ifelse(RMSE <=0.8649, "#006600","red"))
  ) %>%
  knitr::kable("latex", caption = "Comparison of 4 models", 
               escape = FALSE, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = F , position = "center") %>%
  row_spec(0, bold = TRUE, color = "white" , background ="red") %>%
  footnote(general = "Improvement in percentage", 
           general_title = "Note:", footnote_as_chunk = TRUE)
```

The impact of time bias seems to be insignificant. We see an improvement of only $`r improvement`$% which doesn't help us achieve our target RMSE of ***0.8649***. Consequently we would be ignoring this model and would verify the impact of genre effect on Movie + User model instead.

### Modeling movie + user + genre effect 

```{r movie-user-genre-effect-model, cache = TRUE, include = TRUE}

# Modeling movie effect, user effect and genre effect (b_i + b_u + b_g)

# Calculate the genre effect (b_g)
genre_avgs <- genre_train_set %>%
  # Combining movie averages with the genre train set
   left_join(movie_avgs, by = "movieId") %>% 
  # Combining user averages with the genre train set
   left_join(user_avgs, by = "userId") %>% 
   group_by(genres) %>%
   summarise(b_g = mean(rating - mu - b_i - b_u))

# Obtain the prediction of movie ratings
ge_rating <- genre_test_set %>%
  # Combining movie averages with the genre test set
  left_join(movie_avgs, by = "movieId") %>% 
  # Combining user averages with the genre test set
  left_join(user_avgs, by = "userId") %>% 
  # Combining genre averages with the genre test set
  left_join(genre_avgs, by ="genres") %>% 
  mutate(ger = mu + b_i + b_u + b_g)%>%
  pull(ger)

# Evaluate the performance
model_4_rmse <- RMSE(genre_test_set$rating, ge_rating)

# Calculate the improvement in percentage
improvement <- round((model_2_rmse - model_4_rmse) * 100 / model_2_rmse, 4)

# Append the result for visual comparison
rmse_results <- bind_rows(rmse_results, 
                          data_frame(Method = "Movie + User + Genre effect", 
                                     RMSE = round(model_4_rmse, 7),
                                     Improvement = as.character(improvement)))

# Display the results in a tabular format
rmse_results %>%
   mutate(
    RMSE = cell_spec(RMSE, "latex", color = ifelse(RMSE <=0.8649, "#006600","red"))
  ) %>%
  knitr::kable("latex", caption = "Comparison of 5 models", 
               escape = FALSE, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = F , position = "center") %>%
  row_spec(0, bold = TRUE, color = "white" , background ="red") %>%
  row_spec(4, strikeout = TRUE) %>%
  footnote(general = "Improvement in percentage", 
           general_title = "Note:", footnote_as_chunk = TRUE)
```

This model gives us a much better improvement as compared to that of the previous model. We see an improvement of $`r improvement`$%. This is by far the best of our models as it provides an RMSE that actually exceeds our target of ***0.8649***. Now let's apply regularization to this model and see how much of an improvement that brings.

### Regularized movie + user + genre effect  

```{r regularized-movie-user-genre-effect-model-1, cache = TRUE, include = TRUE}

# Penalized Least Square

# Create a set of possible lambdas 
lambdas <- seq(0, 10, 0.25)

mu <- mean(genre_train_set$rating)

# Using cross validation to obtain the lambda that minimizes the RMSE
# Calculate the RMSE for each of the effects or each lambda

rmses <- sapply(lambdas, function(lambda){
  b_i <- train_set %>%
   group_by(movieId) %>%
   summarize(b_i = sum(rating - mu) / (n() + lambda))
  
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n() + lambda))
  
  b_g <- genre_train_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mu - b_i - b_u)/(n() + lambda))
  
# Predict the ratings based on the penalized effects
  predicted_ratings <- genre_test_set %>%
    left_join(b_i, by = "movieId") %>% # Combining movie averages with genre test set
    left_join(b_u, by = "userId") %>% # Combining user averages with the genre test set
    left_join(b_g, by ="genres") %>%
    mutate(ger = mu + b_i + b_u + b_g)%>%
    pull(ger)
  
  return(RMSE(predicted_ratings, genre_test_set$rating))
  
})

# Determine which value of lambda provides the minimum RMSE
lambda <- lambdas[which.min(rmses)]

```

As we can see $\lambda = `r lambda`$ provides the minimum RMSE $`r min(rmses)`$. The plot below depicts $\lambda$ and the RMSE it generates. The $\lambda$ for minimum RMSE is highlighted.

```{r lambda-rmses-plot, echo = FALSE, include = TRUE}

ggplot(data_frame(lambdas, rmses)) +
  geom_point(aes(lambdas, rmses), color = "red") +
  labs(title = "Penalized Least Squares", 
       subtitle = "Lambda that minimizes RMSE") +
  theme(plot.title = element_text(color = "red", face = "bold", hjust = 0.5),
        plot.subtitle = element_text(color = "blue", size = 10, hjust = 0.5)) +
  gghighlight::gghighlight(lambdas == lambdas[which.min(rmses)]) +
  geom_segment(x = -1, xend = lambdas[which.min(rmses)], 
               y = min(rmses), yend = min(rmses), 
               color = "red", lty = 2) +
  geom_segment(x = lambdas[which.min(rmses)], xend = lambdas[which.min(rmses)], 
               y = 0, yend = min(rmses), 
               color = "red", lty = 2) +
  ggrepel::geom_label_repel(aes(lambdas, rmses), label = lambda, 
                            fill = "red", color = "white", 
                            fontface = "bold", hjust = 1, vjust = 1)

```

Let's calculate the improvement based on the RMSE of `r min(rmses)`

```{r regularized-movie-user-genre-effect-model-2, include = TRUE}

# Calculate the improvement in percentage
improvement <- round((model_4_rmse - min(rmses)) * 100 / model_4_rmse, 4)

# Append the result for visual comparison
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Regularized Movie + User + Genre Effect",  
                                     RMSE = round(min(rmses), 7),
                                     Improvement = as.character(improvement)))

#Display the results in a tabular format
rmse_results %>%
   mutate(
    RMSE = cell_spec(RMSE, "latex", color = ifelse(RMSE <=0.8649, "#006600","red"))
  ) %>%
  knitr::kable("latex", caption = "Comparison of 6 models", 
               escape = FALSE, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = F , position = "center") %>%
  row_spec(0, bold = TRUE, color = "white" , background ="red") %>%
  row_spec(4, strikeout = TRUE) %>%
  row_spec(6, bold = TRUE, color = "blue") %>%
  footnote(general = "Improvement in percentage", 
           general_title = "Note:", footnote_as_chunk = TRUE)

```

The improvement of the regularized model is $`r improvement`$%. Note though, that we have so far been applying our models in the train and test sets we created for the purpose by splitting the edx dataset. We are now ready to apply the model on the full edx set. 

## Final Result  

For the final result of our project we would apply the **Regularized Movie + User + Genre Effect Model** to the full edx dataset and use the validation dataset to arrive at the final RMSE.  

```{r final-results, include = TRUE, cache = TRUE}
# Create a set of possible lambdas 
lambdas <- seq(0, 10, 0.25)

# Calculating the various penalized effects based on the lambda that minimizes the RMSE
rmses <- sapply(lambdas, function(lambda){
b_i <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu) / (n() + lambda))

b_u <- edx %>%
  left_join(b_i, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - mu - b_i)/(n() + lambda))

b_g <- genre_edx %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  group_by(genres) %>%
  summarize(b_g = sum(rating - mu - b_i - b_u)/(n() + lambda))

# Obtain the prediction of movie ratings
predicted_ratings <- genre_validation %>%
  left_join(b_i, by = "movieId") %>% # Combining movie averages with the validation set
  left_join(b_u, by = "userId") %>% # Combining user averages with the validation set
  left_join(b_g, by ="genres") %>% # Combining genre averages with the validation set
  mutate(ger = mu + b_i + b_u + b_g)%>%
  pull(ger)

return(RMSE(predicted_ratings, genre_validation$rating))
})

# Determine which value of lambda provides the minimum RMSE
lambda <- lambdas[which.min(rmses)]

```

So $\lambda = `r lambda`$ provides the minimum RMSE $`r min(rmses)`$. The plot below depicts $\lambda$ and the RMSE it generates. The $\lambda$ for minimum RMSE is highlighted

```{r final-rmses-plot, echo = FALSE, include = TRUE}

xintercept <- lambdas[which.min(rmses)]
yintercept <- min(rmses)

ggplot(data_frame(lambdas, rmses)) +
  geom_point(aes(lambdas, rmses), color = "red") +
  labs(title = "Penalized Least Squares", 
       subtitle = "Lambda that minimizes RMSE") +
  theme(plot.title = element_text(color = "red", face = "bold", hjust = 0.5),
        plot.subtitle = element_text(color = "blue", size = 10, hjust = 0.5)) +
  gghighlight::gghighlight(lambdas == lambdas[which.min(rmses)]) +
  geom_segment(x = -1, xend = xintercept, y = yintercept, yend = yintercept, 
               color = "red", lty = 2) +
  geom_segment(x = xintercept, xend = xintercept, y = 0, yend = yintercept, 
               color = "red", lty = 2) +
  ggrepel::geom_label_repel(aes(lambdas, rmses), label = lambda, 
                            fill = "red", color = "white", 
                            fontface = "bold", hjust = 1, vjust = 1)

```

So the final RMSE that we have arrived at is $`r min(rmses)`$ which meets our target of achieving ***RMSE $\le$ 0.8649 ***

```{r final-table, include = TRUE}
final_result <- data_frame(
  Method = "Regularized Movie + User + Genre Effect on full dataset", 
  RMSE = round(min(rmses), 7))

#Display the final results in a tabular format
final_result %>%
  mutate(
    RMSE = cell_spec(RMSE, "latex", color = "white", 
                     background = ifelse(RMSE <=0.8649, "#006600","red"))
  ) %>%
  knitr::kable("latex", escape = FALSE, caption = "Final Result", linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = F , position = "center") %>%
  row_spec(0, bold = TRUE, color = "white" , background ="red") %>%
  row_spec(1, bold = TRUE)
```


# Conclusions  

As part of the project we assesed 6 different models on the stable benchmark 10M movie ratings MovieLens dataset. We trained our models to predict the movie ratings on a subset (train_set) which we had obtained by partitioning the edx dataset. 5 of these were Linear Regression models, we then applied regularization on the best of these 5 to arrive at our 6$^{th}$ model, and proceeded to predict the ratings in the other partition (test_set). Next we determined the RMSE to ascertain the performance of each of these models and compared the RMSEs to arrive at our best model which provided the least RMSE of the lot. Two of the models actually exceeded our target RMSE.

Through this exercise we deduced that the **Regularized Movie + User + Genre Effect Model** was our best model. As a final check we then applied this model on the full **edx** data set and obtained the RMSE on the **validation** data set to obtain the Final RMSE of **$`r min(rmses)`$**.


# References
1. [F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.](http://dx.doi.org/10.1145/2827872)

2. [Introduction to Data Science, Rafael A. Irizarry (2019)](https://rafalab.github.io/dsbook/)

3. [Create Awesome LaTeX Table with knitr::kable and kableExtra, Hao Zhu (2019)](https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf)

4. [An Example R Markdown (2017)](http://www.math.mcgill.ca/yyang/regression/RMarkdown/example.html)

5. [Linear Regression Models](http://people.duke.edu/~rnau/regintro.htm)

6. [Introduction to gghighlight, Hiroaki Yutani (2018) ](https://cran.r-project.org/web/packages/gghighlight/vignettes/gghighlight.html)

[^1]: Source: <https://grouplens.org/datasets/movielens/latest/>; data as of 2019-10-27
[^2]: https://en.wikipedia.org/wiki/Linear_regression
[^3]: [Introduction to Data Science, Rafael A. Irizarry (2019)](https://rafalab.github.io/dsbook/)